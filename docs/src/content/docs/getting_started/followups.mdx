---
title: Followup Calculations
description: Using pre-computed lookup tables for fast exposure time estimates.
---

import {Aside, Steps, Tabs, TabItem} from "@astrojs/starlight/components";

## Overview

For large-scale gravitational wave (GW) followup campaigns, computing sensitivity curves and observation times individually for thousands of events can be computationally expensive. The **followup module** provides fast exposure time estimates using **pre-computed lookup tables** via interpolation.

<Aside type="tip">
  Followup calculations can be **orders of magnitude faster** than full
  simulations, making them ideal for: - Real-time GW alert response - Large
  parameter space studies - Quick feasibility checks
</Aside>

## Pre-computed Lookup Tables

A lookup table contains pre-simulated observation times for a grid of:

- **GW events** (from catalogs like O5 simulations)
- **Delay times** (time from event to observation start)
- **Observatory configurations** (site, zenith, azimuth, IRF version)
- **EBL models**

These tables are typically stored as **Parquet** or **CSV** files.

### Typical Lookup Table Structure

```python
import pandas as pd

# Load lookup table
df = pd.read_parquet("O5_gammapy_observations_v4.parquet")

print(df.columns)
# Columns typically include:
# - coinc_event_id: GW event identifier
# - site: 'north' or 'south'
# - zenith: zenith angle (degrees)
# - delay: observation delay time
# - obs_time: required observation time
# - ebl: EBL model name
# - config: telescope configuration
# - duration: IRF duration
# - seen: whether detection is possible
# ... plus source metadata (long, lat, eiso, dist, etc.)
```

<Aside type="caution">
  Pre-computed lookup tables are **not included** with sensipy. Contact the
  maintainers if you need access to CTA O5 observation catalogs.
</Aside>

## The `get_exposure` Function

The main function for followup calculations is `sensipy.followup.get_exposure()`:

```python
from sensipy import followup
import astropy.units as u
import pandas as pd

# Load lookup table (pre-computed observations)
lookup_df = pd.read_parquet("./O5_gammapy_observations_v4.parquet")

# Get exposure for a specific event
result = followup.get_exposure(
    event_id=1,
    delay=10 * u.s,
    site="north",
    zenith=60,
    extrapolation_df=lookup_df,
    ebl="franceschini",
)

print(result)
```

### Function Parameters

| Parameter          | Type                    | Description                                             |
| ------------------ | ----------------------- | ------------------------------------------------------- |
| `event_id`         | `int`                   | GW event identifier                                     |
| `delay`            | `u.Quantity`            | Observation delay from event onset                      |
| `site`             | `str`                   | Observatory site (`"north"` or `"south"`)               |
| `zenith`           | `int`                   | Zenith angle (degrees, typically 20, 40, 60)            |
| `extrapolation_df` | `pd.DataFrame` or `str` | Lookup table (DataFrame or filepath)                    |
| `ebl`              | `str` or `bool`         | EBL model name, or `True`/`False`                       |
| `config`           | `str`                   | Telescope configuration (default: `"alpha"`)            |
| `duration`         | `int`                   | IRF duration in seconds (default: `1800`)               |
| `event_id_column`  | `str`                   | Column name for event IDs (default: `"coinc_event_id"`) |

### Return Value

Returns a dictionary similar to `Source.observe()`:

```python
{
    'obs_time': <Quantity>,      # Interpolated observation time
   'seen': bool,                 # Detection possible
    'start_time': <Quantity>,    # Observation start (= delay)
    'end_time': <Quantity>,      # start_time + obs_time
    'ebl_model': str,            # EBL model used
    'min_energy': <Quantity>,    # Energy range
    'max_energy': <Quantity>,

    # Source metadata from lookup table
    'long': <Quantity>,
    'lat': <Quantity>,
    'eiso': <Quantity>,
    'dist': <Quantity>,
    'angle': <Quantity>,
    'id': int,

    'error_message': str,        # If any issues
}
```

## Interpolation Method

The `get_exposure` function uses **logarithmic interpolation** to estimate observation times at arbitrary delays:

<Steps>

1. **Filter lookup table** for matching:

   - Event ID
   - Site
   - Zenith angle
   - EBL model
   - Configuration
   - Duration

2. **Extract delay-observation time pairs** from filtered rows

3. **Perform log-log interpolation**:

   ```
   log(obs_time) = f(log(delay))
   ```

4. **Extrapolate if necessary** using linear trend in log-space

5. **Return result** with interpolated observation time

</Steps>

<Aside type="note">
  Logarithmic interpolation is used because both delay times and observation
  times typically span many orders of magnitude and have power-law-like
  relationships.
</Aside>

## Usage Examples

### Basic Followup Query

<Tabs>
  <TabItem label="Simple Query">
    ```python
    from sensipy import followup
    import astropy.units as u
    import pandas as pd

    # Load lookup table once (cache for multiple queries)
    lookup_df = pd.read_parquet("./O5_gammapy_observations_v4.parquet")

    # Query a single event
    result = followup.get_exposure(
        event_id=42,
        delay=30 * u.min,
        site="south",
        zenith=20,
        extrapolation_df=lookup_df,
        ebl="franceschini",
    )

    if result['seen']:
        print(f"Event 42 detectable in {result['obs_time']}")
        print(f"  Start: {result['start_time']}")
        print(f"  End: {result['end_time']}")
    else:
        print(f"Event 42 not detectable")
    ```

  </TabItem>
  
  <TabItem label="Multiple Events">
    ```python
    from sensipy import followup
    import astropy.units as u
    import pandas as pd

    lookup_df = pd.read_parquet("./O5_gammapy_observations_v4.parquet")

    # Query multiple events
    event_ids = [1, 5, 10, 42, 100]
    delay = 10 * u.s
    results = []

    for event_id in event_ids:
        result = followup.get_exposure(
            event_id=event_id,
            delay=delay,
            site="north",
            zenith=40,
            extrapolation_df=lookup_df,
            ebl="franceschini",
        )
        results.append(result)

    # Analyze results
    detectable = [r for r in results if r['seen']]
    print(f"Detectable: {len(detectable)}/{len(results)}")

    if detectable:
        obs_times = [r['obs_time'].to(u.s).value for r in detectable]
        print(f"Median obs time: {np.median(obs_times):.1f} s")
    ```

  </TabItem>
  
  <TabItem label="Delay Scan">
    ```python
    from sensipy import followup
    import astropy.units as u
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt

    lookup_df = pd.read_parquet("./O5_gammapy_observations_v4.parquet")

    # Scan over delays for one event
    event_id = 10
    delays = np.logspace(0, 4, 30) * u.s  # 1s to 10,000s
    obs_times = []

    for delay in delays:
        result = followup.get_exposure(
            event_id=event_id,
            delay=delay,
            site="south",
            zenith=20,
            extrapolation_df=lookup_df,
            ebl="franceschini",
        )

        if result['seen']:
            obs_times.append(result['obs_time'].to(u.s).value)
        else:
            obs_times.append(np.nan)

    # Plot
    plt.figure(figsize=(10, 6))
    plt.loglog(delays.value, obs_times, 'o-')
    plt.xlabel("Delay [s]")
    plt.ylabel("Observation Time [s]")
    plt.title(f"Event {event_id} Detectability")
    plt.grid(True, alpha=0.3)
    plt.show()
    ```

  </TabItem>
</Tabs>

### Parameter Space Exploration

Use followup calculations to explore how detectability varies across parameter space:

```python
from sensipy import followup
import astropy.units as u
import pandas as pd
import numpy as np

lookup_df = pd.read_parquet("./O5_gammapy_observations_v4.parquet")

# Get all unique event IDs
event_ids = lookup_df['coinc_event_id'].unique()[:100]  # First 100 events

# Fixed observing parameters
delay = 60 * u.s
sites = ["north", "south"]
zeniths = [20, 40, 60]

detection_matrix = np.zeros((len(sites), len(zeniths)))

for i, site in enumerate(sites):
    for j, zenith in enumerate(zeniths):
        detections = 0
        for event_id in event_ids:
            result = followup.get_exposure(
                event_id=event_id,
                delay=delay,
                site=site,
                zenith=zenith,
                extrapolation_df=lookup_df,
                ebl="franceschini",
            )
            if result['seen']:
                detections += 1

        detection_matrix[i, j] = detections / len(event_ids)

# detection_matrix now contains detection fractions for each configuration
print("Detection fractions:")
print(f"              Z=20    Z=40    Z=60")
for i, site in enumerate(sites):
    print(f"{site:6s}:   {detection_matrix[i, 0]:.3f}  {detection_matrix[i, 1]:.3f}  {detection_matrix[i, 2]:.3f}")
```

## Helper Functions

### `get_row`: Retrieve Lookup Table Entries

Directly retrieve rows from the lookup table:

```python
from sensipy.followup import get_row
import pandas as pd

lookup_df = pd.read_parquet("./O5_gammapy_observations_v4.parquet")

# Get specific configuration for an event
row = get_row(
    sens_df=lookup_df,
    event_id=42,
    site="south",
    zenith=20,
    ebl=True,  # Use EBL (any model)
    config="alpha",
    duration=1800,
)

print(row)
```

### `extrapolate_obs_time`: Low-level Interpolation

Perform interpolation directly (rarely needed by users):

```python
from sensipy.followup import extrapolate_obs_time
import astropy.units as u

# Manually perform interpolation
result = extrapolate_obs_time(
    event_id=42,
    delay=100 * u.s,
    extrapolation_df=lookup_df,
    filters={'site': 'south', 'zenith': 20, 'ebl': 'franceschini'},
    other_info=['long', 'lat', 'eiso'],  # Additional fields to include
)
```

### `get_sensitivity`: Create Sensitivity from Lookup Table

If you have pre-computed sensitivity curves in your lookup table, you can create `Sensitivity` objects:

```python
from sensipy.followup import get_sensitivity
from sensipy.sensitivity import SensitivityGammapy
import astropy.units as u

# Create sensitivity from lookup table curves
sens = get_sensitivity(
    event_id=42,
    sens_df=lookup_df,
    site="south",
    zenith=20,
    ebl="franceschini",
    config="alpha",
    duration=1800,
    radius=3.0 * u.deg,
    min_energy=0.02 * u.TeV,
    max_energy=10 * u.TeV,
)

# Now use this sensitivity for custom calculations
# (though usually get_exposure is sufficient)
```

## Performance Considerations

Followup calculations are designed for speed:

| Operation     | Typical Time | Use Case         |
| ------------- | ------------ | ---------------- |
| Single query  | ~1-10 ms     | Real-time alerts |
| 100 events    | ~0.1-1 s     | Quick scans      |
| 1000 events   | ~1-10 s      | Catalog analysis |
| 10,000 events | ~10-100 s    | Full O5 catalog  |

<Aside type="tip">
  **Performance tips:** - Load the DataFrame once and reuse it (don't reload
  from file each time) - Use Parquet format for faster loading than CSV - Filter
  the DataFrame before passing to `get_exposure` if you only need a subset
</Aside>

### Caching Lookup Tables

```python
import pandas as pd

# Load once at the start of your script/notebook
LOOKUP_TABLE = pd.read_parquet("./O5_gammapy_observations_v4.parquet")

# Reuse for all queries
def quick_query(event_id, delay):
    return followup.get_exposure(
        event_id=event_id,
        delay=delay,
        site="south",
        zenith=20,
        extrapolation_df=LOOKUP_TABLE,  # Reuse cached table
        ebl="franceschini",
    )
```

## Comparison: Followup vs. Full Simulation

| Aspect           | `get_exposure()` (Followup) | `Source.observe()` (Full Sim) |
| ---------------- | --------------------------- | ----------------------------- |
| **Speed**        | Very fast (ms)              | Slow (seconds to minutes)     |
| **Accuracy**     | Interpolated                | Exact                         |
| **Flexibility**  | Limited to table params     | Fully customizable            |
| **Requirements** | Pre-computed table          | IRF files, spectral models    |
| **Use case**     | Quick lookups, real-time    | Detailed studies, new configs |

<Aside type="note">
  Use followup calculations for **quick estimates** and **parameter scans**. Use
  full simulations when you need **precise results** or are working with
  **custom configurations** not in the lookup table.
</Aside>

## Creating Your Own Lookup Tables

To create a lookup table for your own catalog:

```python
from sensipy.ctaoirf import IRFHouse
from sensipy.sensitivity import SensitivityGammapy
from sensipy.source import Source
import astropy.units as u
import pandas as pd
from pathlib import Path

# Load IRFs
house = IRFHouse(base_directory="./CTA-IRFs")

# Configuration grid
sites = ["north", "south"]
zeniths = [20, 40, 60]
delays = [10, 30, 100, 300, 1000, 3000] * u.s
ebl_models = [None, "franceschini"]

# Catalog of events
event_files = list(Path("./catalog/").glob("*.fits"))

results = []

for event_file in event_files:
    for site in sites:
        for zenith in zeniths:
            for ebl in ebl_models:
                # Load IRF
                irf = house.get_irf(
                    site=site,
                    configuration="alpha",
                    zenith=zenith,
                    duration=1800,
                    azimuth="average",
                    version="prod5-v0.1",
                )

                # Load source
                source = Source(
                    filepath=str(event_file),
                    min_energy=20 * u.GeV,
                    max_energy=10 * u.TeV,
                    ebl=ebl
                )

                # Calculate sensitivity
                sens = SensitivityGammapy(
                    irf=irf,
                    observatory=f"ctao_{site}",
                    min_energy=20 * u.GeV,
                    max_energy=10 * u.TeV,
                    radius=3.0 * u.deg,
                )
                sens.get_sensitivity_curve(grb=source)

                # Simulate at each delay
                for delay in delays:
                    result = source.observe(
                        sensitivity=sens,
                        start_time=delay,
                        min_energy=20 * u.GeV,
                        max_energy=10 * u.TeV,
                    )

                    # Add configuration info
                    result['site'] = site
                    result['zenith'] = zenith
                    result['delay'] = delay.to(u.s).value
                    result['ebl'] = ebl if ebl else "none"

                    results.append(result)

# Save to Parquet
df = pd.DataFrame(results)
df.to_parquet("my_lookup_table.parquet")
```

## Troubleshooting

### Common Issues

**Problem**: "No matching rows found in lookup table"

**Solution**:

- Verify the event ID exists in the table
- Check that site/zenith/EBL combination exists
- Use `get_row()` to debug what's available

---

**Problem**: Interpolation returns unrealistic values

**Solution**:

- Check that the lookup table has sufficient delay points for interpolation
- Verify the delay range covers your query point
- The table may not have data for this eventâ€”try a different one

---

**Problem**: Very slow performance

**Solution**:

- Load the DataFrame once and cache it (don't reload from file repeatedly)
- Use Parquet instead of CSV
- Pre-filter the DataFrame to reduce size

## Next Steps

- See the [Tutorials](/tutorials/) for complete workflow examples
- Check the [Followup API reference](/reference/followup) for detailed documentation
- Learn about [Sensitivity Calculations](./sensitivity) to understand the underlying methods
